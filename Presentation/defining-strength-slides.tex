\documentclass[xcolor={dvipsnames,table}]{beamer}


\mode<presentation>
{%
  \usetheme{Warsaw}
  % or ...
  \usecolortheme{dolphin}

  \setbeamercovered{transparent}
  % or whatever (possibly just delete it)
}

\mode<handout>
{%
  \usecolortheme{dove}

  \usepackage{pgf4up}

  \pgfpagesuselayout{4 on 1 boxed}[letterpaper, border shrink=5mm, landscape]
}

\usepackage{polyglossia}
\setdefaultlanguage{english}
\usepackage{fontspec}
\usepackage{xltxtra}
\usepackage{hyperref}

\usepackage{tikz}

\newcommand\RBox[1]{%
  \tikz\node[draw,rounded corners,align=center,] {#1};
}
\setbeamerfont{author in head/foot}{size={\fontsize{3pt}{4pt}\selectfont}}

\title{Defining Password Strength}

\author{Jeffrey Goldberg}
\institute[AgileBits] % (optional, but mostly needed)
{%
  Chief Defender Against the Dark Arts\\
  AgileBits Inc.\\
  \texttt{jeff@agilebits.com}}

\date[Password13]{Passwords13}

%\subject{Theoretical Computer Science}
% This is only inserted into the PDF information catalog. Can be left
% out.


\pgfdeclareimage[height=0.5cm]{agilebits-logo}{AgileBits-white-300.png}
\logo{\pgfuseimage{agilebits-logo}}


% If you wish to uncover everything in a step-wise fashion, uncomment
% the following command:

%\beamerdefaultoverlayspecification{<+->}

\newcommand\pwd[1]{\texttt{#1}}
\newcommand\func[1]{\ensuremath{#1(\cdot)}}
%\newcommand\G{\ensuremath{\mathcal{G}}}
\newcommand\G{\ensuremath{\Gamma}}


\newcommand\tquote[1]{``\textit{\textcolor{Plum}{#1}}''}
\newcommand\itsok[1]{\vspace{0.5ex} \textcolor{Mahogany}{\colorbox{Goldenrod!20}{It's okay} because #1}}

%\usepackage[american]{babel}
\usepackage{csquotes}
\usepackage[style=authoryear,backend=biber]{biblatex}
\DeclareLanguageMapping{american}{american-apa}

% generated with 
%   biber --output_format=bibtex --output_resolve defining-strength-slides.bcf
\addbibresource{defining-strength-slides_biber.bib}
%\addbibresource{crypto.bib}



\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Outline}
  \tableofcontents
  % You might wish to add the option [pausesections]
\end{frame}

\section{Why define password strength?}

%\begin{frame}{Reasons \emph{Not} to Define Password Strength}
%  \begin{itemize}
%  \pause
%  \item
%    People might think that reliable strength meters are feasible.
%  \pause
%  \item
%    In practice, strength depends on the attacking technology, which is subject to change
%  \end{itemize}
%\end{frame}

\begin{frame}{Fools rush in \dots }
\framesubtitle{\dots where angels fear to tread}
\begin{quote}
Fools rush in where angels fear to tread.
\end{quote}
\end{frame}

\begin{frame}{Reasons \emph{To} Define Password Strength}

  \begin{itemize}
  \item We informally talk about password strength all the time. E.g.
    \begin{itemize}
    \pause
    \item \tquote{\pwd{olByWo9yIFp8NfOxSprJXX} is a stronger password than  \pwd{Password1}.}
    \pause
    \item \tquote{\pwd{12345}? That's terrible, that is the kind of thing an idiot would put on his luggage.}
    \end{itemize}
  \pause
  \item We like to compare password strength to other parts of the system. E.g.,
    \begin{itemize}
    \pause
    \item \tquote{Should you really be worrying of the difference between 128-bit AES keys and 256-bit AES keys when your password is probably less than 40 bits?}
    \end{itemize}
   \end{itemize}
\end{frame}

\section{Previous attempts}
\begin{frame}{Uniform distribution}
A "uniform distribution" is one in which all elements are equally likely.

\begin{itemize}
\item[Uniform]
 \begin{itemize}
 \pause
 \item The result of rolling a single (fair) die
 \pause
 \item AES keys generated by non-terrible random number generator
 \pause
 \item Passwords generated by a good password generator
 \end{itemize}

\item[Not]
 \begin{itemize}
 \pause
 \item The sum of rolling a pair of dice
 \pause
 \item Word frequencies in human language
 \pause
 \item Passwords generated by humans
 \end{itemize}
\end{itemize}
\end{frame}



\subsection{Shannon Entropy}

\begin{frame}{Shannon Entropy, \(\func{H}\).}
\begin{definition}[Shannon Entropy]
The entropy, $H(X)$, of a discrete random distribution, $X$, is
$$H(X) = -\sum_{i=1}^n p(x_i)\log_2p(x_i)$$
where $x_i$ is the $i$th element of $X$, and $p(x_i)$ is the probability of selecting $x_i$.
\end{definition}
\end{frame}

\begin{frame}{When Shannon Entropy Is Good \dots }
\framesubtitle{\dots it is very very good.}
\begin{itemize}
\pause
\item It's familiar (almost everyone talks in these terms)
\pause
\item Its units are bits. Yeah bits!
\pause
\item It is well understood.
\pause
\item It is commensurate with other systems.
\pause
\item Each bit doubles cracking time
\end{itemize}
\pause
\alert{Only ``good'' when passwords are distributed uniformly}
\end{frame}

\begin{frame}{But when it is bad \dots}
\nocite{WeirETAL2010:CCS}
\dots it is horrid
\begin{itemize}
 \item When the distribution is not a uniform distribution, Shannon entropy can yield meaningless results
\end{itemize}
\begin{quote}
Even with an accurate Shannon entropy value, it would not tell the defender anything about how vulnerable a system would be to an online password cracking attack.
 [\cite[p.~162]{WeirETAL2010:CCS}]
\end{quote}
\end{frame}

\subsection{Guessing Entropy}

\begin{frame}{Guessing Entropy}
Guessing Entropy is the average number guesses to find $x_i$ in $X$ when $X$ is sorted by likeliness.
\begin{definition}[Guessing Entropy]
The Guessing Entropy, $G(X)$ of a distribution $X$ where the values of $X$ are sorted by decreasing probability, so that if $i > j$ then $p(x_i) \geq p(x_j)$,
$$
G(X) = \sum_{i=0}^{\max(R(X))} p(x_i)(i+1)
$$
\end{definition}
[Following formalization of \cite{Cederlog2005:Thesis}]
\end{frame}

\begin{frame}{When Guessing: The Good and the Bad}
\begin{itemize}
\item[Good]
Guessing Entropy is rooted in the number of guesses it takes to find a password.

\item[Bad]
\pause
\begin{itemize}
\item Not measured in bits

      \pause
      (Easy to fix)
\pause
\item Has the same problems as $\func{H}$ with fat headed distributions.

      \pause
      (Really!)
\end{itemize}
\end{itemize}
\end{frame}

\subsection{Min-entropy}

\begin{frame}{Min-entropy}
Min-entropy is is based solely on the probability of the most likely value in $X$.
\begin{definition}[min-entropy]\label{def:minH}
The min-entropy, $H_\infty$, of a distribution, $X$, is the negative base 2 logarithm of the probability of a most probable value in $X$.
$$
H_\infty(X) = -\log_2 p(x_m)
$$
where $x_m$ has the maximum probability in $X$, that is, $\forall x_i \in X, p(x_m) \geq p(x_i)$
\end{definition}
[Min-entropy is a special case of RÃ©nyi Entropy, so I use that notation]
\end{frame}

\begin{frame}{Our Best Worst Case}
\framesubtitle{Min-entropy can be useful}
Although min-entropy throws away an enormous amount of information about the distribution, it may be the most useful entropy notion when talking about distributions of passwords, as it is based solely on the worst case.

\end{frame}

\section{Why Entropy fails}
\subsection{Fat-headed distributions}

\begin{frame}{What has a fat head and a long tail?}
Both Guessing Entropy and Shannon Entropy fail for talking about password strength when passwords are distributed with a fat head (a few elements that are very likely) and a long thin tail (lots of low probability elements).
\end{frame}

\begin{frame}{An extreme example}
Imagine if a value, $q_0$, shows up 9 times out of 10. The remaining 1 time out of 10, it is one of $2^{512}$ possibilities. This distribution, $Q$, has one very likely element, and lots of of unlikely elements.

\begin{definition}[Troublesome distribution, $Q$] \label{def:Q}
Let $Q$ be a distribution of $2^{512}+1$ values. $q_0$ has a probability of $0.9$ and all of the other values, $q_1 \dots q_n$,
have a probability of $(1-0.9)2^{-512}$.
\end{definition}
\end{frame}

\begin{frame}{\(Q\) and Entropy Results}
\begin{center}\large
What different entropy notions do with $Q$
\rowcolors{1}{RoyalBlue!20}{RoyalBlue!5}
\begin{tabular}{|lr|}
\hline
Shannon Entropy & $H(Q) \approx 51.66$ bits \\
Guessing Entropy &$G(Q) \approx 2^{507.6}$ guesses \\
Min-entropy &$H_\infty(Q) \approx 0.15$ bits \\
\hline
\end{tabular}
\end{center}
\end{frame}

\subsection{Calculations}

\begin{frame}{Shannon Entropy and \(Q\) }

\begin{align}
H(Q) &= -\left[ 0.9\log_2 0.9
       + \sum_1^{2^{512}}\left. (1-0.9) 2^{-512}\log_2\left((1-0.9)2^{-512}\right)\right.\right]\notag \\
     &\approx  0.13 + 51.53\notag \\
     &\approx 51.66 \notag
\end{align}
\end{frame}

\begin{frame}{Guessing Entropy and \(Q\) }
\begin{align}
G(Q) &= p(q_0) + \sum_{i=1}^{2^{512}} p(q_i)(i+1) \notag \\
     & = 0.9 +  \frac{1-0.9}{2^{512}} \cdot \sum_{j=2}^{2^{512}+1}j \notag \\
     &= 0.9 + \frac{0.1}{2^{512}} \cdot \frac{2^{512}(2^{512} +3)}{2} \notag \\
     &= 0.9 + \frac{2^{512} + 3}{20}\notag \\
     &\approx 2^{507.6} \notag
\end{align}

\end{frame}

%\begin{frame}{Min-entropy and \(Q\) }
%\begin{align}
%H_\infty(Q) &= -\log_2 0.9 \notag \\
%
%            &\approx 0.15 \notag
%\end{align}
%\end{frame}

\subsection{What we learn from fat heads}

\begin{frame}{The trouble with ignoring the password}

Using a single statistic (some form of Entropy) for a distribution
\begin{itemize}
\item Throws out too much information
\item Aims for the ``average'' password (under various notions of ``average'')
\item Gets distorted results when the distribution is far from uniform
\end{itemize}
Instead, we should give up on a statistic for a distribution and look at the strength of a particular password with respect to a distribution.
\end{frame}

\section{Getting Particular}

\subsection{Definitions}

\begin{frame}{Guess for a particular password}
\begin{definition}[Guesses Function, \(\G\)] \label{def:calG}
$\G(p, X, k)$ is the averages number of guesses that the best algorithm needs to find $k$ in $X$ with probability $p$, where $X$ is a discrete probability distribution, $k$ is a value in $X$, and $p$ is a probability $0 \leq p \leq 1$.
\end{definition}
\end{frame}

\begin{frame}{The arguments of \(\G\)}
$\G$ is a function of three arguments.
\begin{description}
\item[$X$] The distribution the password is drawn from is crucial to how many guesses are needed to find it.
\item[$k$] The password you are looking for within the distribution matters for the number of guesses
\item[$p$] Your target probability of finding the password after a number of guesses.
\end{description}
\end{frame}

\begin{frame}{Definition of password strength}
\begin{definition}[Password Strength] \label{def:S}
The strength of a password, $w$, with respect to a distribution, $X$ is given by
$$
S(w, X) =  1 + \log_2\G(0.5, X, w)
$$
\end{definition}
This is just the average number of guesses to have a 50\% chance of finding the password, $w$, in some distribution. It is manipulated to have a result in bits.
\end{frame}

\subsection{What this buys us}

\begin{frame}{Not a Big Deal, But \dots}

\begin{itemize}
\item It is a function of \emph{both} the distribution and the password's place within it.

\pause
\item Its units are convenient

\pause
\item We know what we mean when we say Password $w_i$ is stronger than password $w_j$

\pause
\item It reflects how difficult it is to crack the password.
\end{itemize}
\end{frame}

\begin{frame}{Using the Information we need}


The strength of a password is a function of \emph{both}
\begin{itemize}
\item the distribution
\item the password's place within the distribution.
\end{itemize}
\end{frame}



\begin{frame}{Its units are convenient}

Its units are convenient
\begin{itemize}
  \pause \item When $X$ is a uniform distribution $S(w, X) = H(x)$
  \pause \item It can be compared to encryption key sizes
\end{itemize}
\end{frame}


\begin{frame}{We can talk about relative strength}

We know what we mean when we say Password $w_i$ is stronger than password $w_j$
\begin{itemize}
\item when $w_i$ and $w_j$ are drawn from the same distribution
\item when $w_i$ and $w_j$ are drawn from different distributions.
\end{itemize}

\end{frame}

\begin{frame}{Reflects crackability}
This definition reflects how difficult it is to crack the password.

\pause
Well, at least it tries to. There is stuff it ignores.
\end{frame}

\subsection{What this doesn't do}

\begin{frame}{Password strength meters still suck}
\begin{itemize}
\item Definition is not constructive

       If anything, this definition of strength reinforces the notion that the only reliable way at present to gauge the strength of a password is to try to crack it.

\item But sucky strength meters may still be useful.

       There is some experimental evidence that placing (necessarily sucky) password strength meters in some contexts does improve password choice behavior. \parencite{EgelmanETAL13:SIGCHI}
\end{itemize}

\pause
\itsok{I wasn't setting out to build a better password strength meter.}

\end{frame}

\begin{frame}{\(Q\) is contrived}
The example distribution, $Q$, used to illustrate the problem with Shannon and Guessing entropy is contrived.

\pause
\itsok{it still illustrates what is a deep problem with those entropy notions when used for passwords.}

\pause
\itsok{actual distributions may approximate a power-law distribution \parencite{MaloneMaher11:CoRR}, which is also fat headed. (But I haven't done the math on this.)}

\end{frame}


\begin{frame}{``Best Algorithm'' Isn't Always Best}

Definition of $\G$ assumes ``best algorithm'' guesses passwords in order of likeliness. But \dots
\begin{itemize}
\item Fastest crack times may involve proceeding out of order
  \begin{itemize}
  \item Some candidates may take more time to check than others
  \item May be faster to check groups of related candidates together
  \end{itemize}
\item Parallelization makes a hash of taking candidates in sequence
\end{itemize}
So the proposed definition does not reflect actual, practical cracking technology

\pause
\itsok{we shouldn't build our definitions around current technology, which changes rapidly.}

\end{frame}

\begin{frame}{We still don't know distribution}

\begin{itemize}
\item Understanding brains is hard

    Without good models of password choice, our $X$ is still a big unknown.

\item Modeling choice outcomes

    But we can avoid modeling choice if we can model observed distributions.
    E.g., Markov models \parencite[e.g.,][]{NarayananShmatikov:2005:FDA} or Probabilistic Context-free Grammers \parencite[e.g.,][]{WeirETAL2009:ProbCFGs}.
\end{itemize}

\pause
\itsok{it gave me the opportunity the mention some things that I find very interesting.}

\end{frame}




\end{document}
